{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранжирование, основанное на BM25F с двумя зонами - заголовок и основной текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "import Stemmer\n",
    "stemmer_ru = Mystem()\n",
    "stemmer_en = Stemmer.Stemmer('english')\n",
    "import re\n",
    "\n",
    "\n",
    "def Steming(text):\n",
    "    text_out = stemmer_ru.lemmatize(text)\n",
    "    text_out = ''.join(text_out)\n",
    "    text_out = re.sub(re.compile(\"\\W+\"), r' ', text_out)\n",
    "    text_out = stemmer_en.stemWords(text_out.split(\" \"))\n",
    "    return \" \".join(text_out)\n",
    "\n",
    "def Normalize_text(text):\n",
    "    text = text.replace(\"-\\n\", \"\")\n",
    "    text = re.sub(re.compile(\"\\W+\"), r' ', text)\n",
    "    text = \" \".join(re.split(re.compile(\"(\\d+)\"), text))\n",
    "    return text\n",
    "\n",
    "def Normalize_word(word):\n",
    "    word = word.replace(u'ё', u'е')\n",
    "    word = word.replace('_', '')\n",
    "    word = word.strip()\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def is_visible(element):\n",
    "    if element.parent.name in ['[document]', 'head', 'title', 'style', 'script']:\n",
    "        return False\n",
    "    elif re.match(re.compile('<!---.*--->', re.DOTALL), str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "def Extract(text):\n",
    "    text = Normalize_text(text)\n",
    "    text = Steming(text)\n",
    "    out = []\n",
    "    for word in text.split(\" \"):\n",
    "        word = Normalize_word(word)\n",
    "        if word == '':\n",
    "            continue\n",
    "        out.append(word)\n",
    "    return out\n",
    "\n",
    "def CleanFiles(urls, inputs, output, files):\n",
    "    for name in files:\n",
    "        f = open(inputs+\"/\"+name, \"r\", encoding=\"utf-8\", errors='replace')\n",
    "        url = f.readline()[:-1]\n",
    "        text = f.read().lower()\n",
    "        output = output+\"/\"+str(urls[url])\n",
    "        if os.path.exists(output):\n",
    "            return\n",
    "        f = open(output, 'w', encoding='utf-8')\n",
    "        titles = re.search(re.compile('<title>(.*?)<\\/title>', re.DOTALL), text)\n",
    "        title = titles.groups()[0] if titles is not None else ''\n",
    "        text = text.replace(\"<\", \" <\").replace(\">\", \"> \")\n",
    "        text = \" \".join(filter(is_visible, BeautifulSoup(text).findAll(text=True)))\n",
    "        text = \" \".join(Extract(title + \" \" + \"SPLITTER\" + \" \" + text))\n",
    "        title, text = text.split(\"SPLITTER\")\n",
    "        f.write(title+\"\\n\")\n",
    "        f.write(text)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmas:\n",
    "    def __init__(self):\n",
    "        self.docs = 0\n",
    "        self.counter = 0\n",
    "\n",
    "def GenLemmStat(inputs, output, filenames):\n",
    "    for name in filenames:\n",
    "        input_path = inputs+\"/\"+name\n",
    "        output_path = output+\"/\"+name\n",
    "        if os.path.exists(output_path):\n",
    "            return\n",
    "        input_file = open(input_path, 'r', encoding='utf-8')\n",
    "        title = (input_file.readline()[:-1])\n",
    "        inner_text = input_file.read()\n",
    "        input_file.close()\n",
    "        words_dict = defaultdict(list)\n",
    "        for word in title.split(' '):\n",
    "            if word != '':\n",
    "                words_dict[word.lower()].append(0)\n",
    "        skipped = -1\n",
    "        for pos, word in enumerate(inner_text.split(' ')):\n",
    "            if word == '':\n",
    "                skipped += 1\n",
    "            else:\n",
    "                words_dict[word.lower()].append(pos + skipped)\n",
    "        file = open(output_path, 'w', encoding='utf-8')\n",
    "        length = 0\n",
    "        for word, pos in words_dict.items():\n",
    "            final_length += len(pos)\n",
    "            file.write(word + \"\\t\"+str(len(positions)) + \"\\t\")\n",
    "            file.write(\" \".join(str(pos) for pos in positions))\n",
    "            file.write(\"\\n\")\n",
    "        file.write(str(length))\n",
    "        file.close()\n",
    "        \n",
    "def MakeStat(input_dir, out_filename, filenames):\n",
    "    corpus_dict = defaultdict(Lemmas)\n",
    "    for name in filenames:\n",
    "        file = open(input_dir+\"/\"+name, 'r', encoding='utf-8')\n",
    "        words_info = file.read().splitlines()[:-1]\n",
    "        for info in words_info:\n",
    "            info = info.split('\\t')\n",
    "            word = info[0]\n",
    "            corpus_dict[word].counter += int(info[1])\n",
    "            corpus_dict[word].docs += 1\n",
    "        file.close()\n",
    "    out_file = open(out_filename, 'w', encoding='utf-8')\n",
    "    corpus_size = 0\n",
    "    for w, info in corpus_dict.items():\n",
    "        out_file.write(w+\"\\t\"+str(info.docs)+\"\\t\"+str(info.counter)+\"\\n\")\n",
    "        corpus_size += info.counter\n",
    "    out_file.write(str(corpus_size))\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess .dats and count tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKERS = 4\n",
    "URLS = 38114\n",
    "\n",
    "def GetSplit(filenames, workers=WORKERS):\n",
    "    splits = np.linspace(0, len(filenames), workers+1, dtype=np.int)\n",
    "    return [filenames[splits[i]:splits[i+1]] for i in range(workers)]\n",
    "\n",
    "def StartPool(func, args, workers=WORKERS):\n",
    "    proc_pool = Pool(workers)\n",
    "    proc_pool.starmap(func, args)\n",
    "    proc_pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20170702', '20170704', '20170707', '20170708', '20170710', '20170711', '20170717', '20170726']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data/urls.numerate.txt\", 'r', encoding=\"utf8\")\n",
    "urls = [line.split('\\t') for line in f.read().splitlines()]\n",
    "urls = dict((url[1], url[0]) for url in urls)\n",
    "f.close()\n",
    "dates = sorted(os.listdir('./data/content'))\n",
    "print(dates)\n",
    "filenames = []\n",
    "for date in dates:\n",
    "    for doc in sorted(os.listdir('./data/content/' + date)):\n",
    "        filenames.append(date + '/' + doc)\n",
    "splits = GetSplit(filenames)\n",
    "StartPool(CleanFiles, [(urls, 'data/content/', 'data/clean_content', splits[i]) for i in range(WORKERS)])\n",
    "\n",
    "splits = GetSplit([str(i) for i in range(1, URLS+1)])\n",
    "StartPool(GenLemmStat, [('data/clean_content', 'data/tf_content', splits[i]) for i in range(WORKERS)])\n",
    "\n",
    "MakeStat('data/tf_content', 'data/statistics.txt', [str(i) for i in range(1, URLS+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVG_DOC_LEN = 13196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self):\n",
    "        self.docs = []\n",
    "        self.words = ''\n",
    "\n",
    "def GetIDF(words, words_stat, total_len):\n",
    "    result = []\n",
    "    for word in words:\n",
    "        if words_stat[word].counter != 0:\n",
    "            result.append(np.log(total_len / words_stat[word].counter))\n",
    "        else:\n",
    "            result.append(0.0)\n",
    "    return result\n",
    "\n",
    "def GetTF(ids):\n",
    "    document_dict = defaultdict(list)\n",
    "    doc_file = open('data/tf_content/'+str(ids), 'r', encoding='utf-8')\n",
    "    lines = doc_file.read().splitlines()\n",
    "    doc_file.close()\n",
    "    for line in lines[:-1]:\n",
    "        line_parts = line.split('\\t')\n",
    "        document_dict[line_parts[0]] = [int(i) for i in line_parts[2].split(\" \")]\n",
    "    return document_dict, int(lines[-1])\n",
    "\n",
    "def Score(words, words_idf, document_dict, document_len):\n",
    "    score = 0\n",
    "    for word_id, word in enumerate(words):\n",
    "        f = len(document_dict[word])\n",
    "        idf = words_idf[word_id]\n",
    "        score += idf * (f / (f + 1 + 0.001 * document_len) + int(0 in document_dict[word]))\n",
    "    return score\n",
    "\n",
    "def Scoring(query, words_stat, total_len):\n",
    "    data = [GetTF(idx) for idx in query.docs]\n",
    "    words_idf = GetIDF(query.words, words_stat, total_len)\n",
    "    scores = []\n",
    "    for idx in range(len(query.docs)):\n",
    "        scores.append(Score(query.words, words_idf, data[idx][0], data[idx][1]))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CleanWords(text):\n",
    "    words = Steming(Normalize_text(text)).split(\" \")\n",
    "    words_list = [Normalize_word(word).lower() for word in words]\n",
    "    out = []\n",
    "    for word in words_list:\n",
    "        if word == '':\n",
    "            continue\n",
    "        out.append(word)\n",
    "    return out\n",
    "\n",
    "def ProcessQueries():\n",
    "    queries = defaultdict(Query)\n",
    "    f = open(\"data/queries.numerate.txt\", 'r', encoding='utf-8')\n",
    "    for query in f.read().splitlines():\n",
    "        split = query.split('\\t')\n",
    "        queries[int(split[0])].words = CleanWords(split[1])\n",
    "    f.close()\n",
    "    f = open('data/sample.technosphere.ir1.textrelevance.submission.txt', 'r', encoding='utf-8')\n",
    "    for line in f.read().splitlines()[1:]:\n",
    "        split = line.split(\",\")\n",
    "        queries[int(split[0])].docs.append(int(split[1]))\n",
    "    f.close()\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_stat = defaultdict(Lemmas)\n",
    "f = open('data/statistics.txt', 'r', encoding='utf-8')\n",
    "lines = f.read().splitlines()\n",
    "f.close()\n",
    "for line in lines[:-1]:\n",
    "    line = line.split('\\t')\n",
    "    words_stat[line[0]].docs, words_stat[line[0]].counter = int(line[1]), int(line[2])\n",
    "\n",
    "queries_dict = ProcessQueries()\n",
    "\n",
    "submission = open('./submission.txt', 'w')\n",
    "submission.write(\"QueryId,DocumentId\\n\")\n",
    "for query_id, query in queries_dict.items():\n",
    "    scores = Scoring(query, words_stat, int(lines[-1]))\n",
    "    for idx in np.argsort(scores)[::-1]:\n",
    "        submission.write(str(query_id) + \",\" + str(query.docs[idx]) + \"\\n\")\n",
    "submission.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
